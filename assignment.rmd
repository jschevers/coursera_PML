---
title: "Human Activity Recognition"
author: "Jesse Schevers"
date: "Saturday, July 25, 2015"
output: html_document
---
## Introduction
This is the report for the  **Coursera** course *Practical Machine Learning*. Our goal is to predict *how well* a weightlifting exercise is performed. 
We will use data from accelerometers on the belt, forearm, arm, and dumbbel of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.   

More information is available on the [HAR](http://groupware.les.inf.puc-rio.br/har) website.

```{r libs, echo=FALSE,message=FALSE}
library(caret)
library(knitr)
library(ggplot2)
```
 
## The Data
The Weight Lifting Exercise Dataset can be downloaded here:
 
* [training dataset](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) 
* [test dataset](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)
 
There are five *Classes* quantifying what *how well* means:  
 
* Class **A**: exactly according to the specification,
* Class **B**: throwing the elbows to the front, 
* Class **C**: lifting the dumbbell only halfway,
* Class **D**: lowering the dumbbell only halfway,
* Class **E**: throwing the hips to the front  

I will split the (downloaded) training dataset into a trainingset and a validationset. The trainingset will be used to build the model(s), the validationset for cross-validation and the (downloaded) test dataset for the assignment submission.  
We can't use all the variables. There first few variables are about the experiment set-up. The variables with Na's (or NULL's) have only few or no values. In the test dataset, these variables have all missing values. So they will be removed. 
 
 
```{r data,echo=FALSE,cache=TRUE}
# download and reading trainingdata
train_url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
train_csv="pml-training.csv"
if (!file.exists(train_csv)){download.file(train_url,destfile=train_csv)}
training=data.frame(read.csv(train_csv),stringsAsFactors=FALSE)
#set all integer to numeric
for (i in 1:ncol(training))
{ if (class(training[,i])=="integer")
{training[,i]=as.numeric(training[,i])}}
 
# download and reading testdata
test_url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
test_csv="pml-testing.csv"
if (!file.exists(test_csv)){download.file(test_url,destfile=test_csv)}
testing=data.frame(read.csv(test_csv),stringsAsFactors=FALSE)
#set all integer to numeric
for (i in 1:ncol(testing))
{ if (class(testing[,i])=="integer")
{testing[,i]=as.numeric(testing[,i])}}

# create trainset and validationset
set.seed(12345)
inTrain <- createDataPartition(y=training$classe,p=0.8, list=FALSE)
trainset=training[inTrain,]
valset=training[-inTrain,]

## fields to use
fields=c('classe'
         ,'roll_belt','pitch_belt','yaw_belt','total_accel_belt'
         ,'gyros_belt_x','gyros_belt_y','gyros_belt_z'
         ,'accel_belt_x','accel_belt_y','accel_belt_z'
         ,'magnet_belt_x','magnet_belt_y','magnet_belt_z'
         ,'roll_arm','pitch_arm','yaw_arm','total_accel_arm'
         ,'gyros_arm_x','gyros_arm_y','gyros_arm_z'
         ,'accel_arm_x','accel_arm_y','accel_arm_z'
         ,'magnet_arm_x','magnet_arm_y','magnet_arm_z'
         ,'roll_dumbbell','pitch_dumbbell','yaw_dumbbell','total_accel_dumbbell'
         ,'gyros_dumbbell_x','gyros_dumbbell_y','gyros_dumbbell_z'
         ,'accel_dumbbell_x','accel_dumbbell_y','accel_dumbbell_z'
         ,'magnet_dumbbell_x','magnet_dumbbell_y','magnet_dumbbell_z'
         ,'roll_forearm','pitch_forearm','yaw_forearm','total_accel_forearm'
         ,'gyros_forearm_x','gyros_forearm_y','gyros_forearm_z'
         ,'accel_forearm_x','accel_forearm_y','accel_forearm_z'
         ,'magnet_forearm_x','magnet_forearm_y','magnet_forearm_z')
#reduce covariats
trainset=trainset[,fields]
```

The variable we are trying to predict is **classe**. The covariates are:  
```{r cov,echo=FALSE,cache=TRUE}
kable(matrix(fields[-c(1)],ncol=4))
```

## The Model 
Since this is a classification problem I will use tree-based-models. A single tree can easily be interpreted, but the covariates itself are not very interpretable (a lot of sensor-data), so interpretably is not so much of an issue here, therefore a random forest is much more interesting. (of course I did build a single tree (ctree), but I was not a very good predictor)  
First I tried to build a random forest based on conditional inference trees (cforest), but my computer ran out of memory. So a *cforest* is not an option (for me). 
So I will go for a **random forest**. 
  
## Results Random Forest

The trainingset will be used for building the model. I will save the resulting "train"-object on disk and load it when I need it.   
 
```{r rf,echo=TRUE,cache=TRUE,message=FALSE}
#fit_forest= train(y=trainset$classe , x=trainset[,!names(trainset)=="classe"]
#                ,method="rf")
#saveRDS(fit_forest, file="fit_forest.rds")
fit_forest=readRDS("fit_forest.rds")
valset$pred_rf= predict(fit_forest,newdata=valset[,fields])
cM=confusionMatrix(data=valset$pred_rf,reference=valset$classe) 
``` 
 
I used the validationset for cross-validation. Let's take a look at the confusionmatrix for the validationset: 
```{r acc,echo=FALSE,cache=TRUE}
acc=cM$overall["Accuracy"]
oob=round((1-acc),digits=3)
cM
``` 
The *Accuracy* on the validationset is `r round(acc,digits=3)`, so the *out of sample error rate* is 1-`r round(acc,digits=3)`=  `r oob`. Since the validationset is not used for building the model, this is a good estimate for the *out of sample error*.
  
As a bonus, I want to see how the most important covariates relate to *classe*. Let's take a look at the (top 10) variable importance: 
```{r plotvar,echo=FALSE,message=FALSE, fig.height=5,fig.width=12} 
plot(varImp(fit_forest),top=10)
```
*roll_belt* and *yaw_belt* are on top. Let's see how they relate to *classe*: 
```{r plotclas,echo=FALSE,fig.height=8,fig.width=12}
      qplot(roll_belt,yaw_belt,data=valset,color=classe)
```
